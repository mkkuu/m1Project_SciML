{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "# 14 bis - Extraction d'un état réduit par sélection\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 1. Introduction et justification\n",
    "\n",
    "> #### Pourquoi extraire un état réduit de cette manière ?\n",
    "\n",
    "Quand on construit notre état réduit à partir de la PCA/EOF, on construit des combinaisons linéaires des variables originales. Les modes (EOF1, EOF2, etc.) n'existent pas physiquement et permettent surtout de maximiser la variance expliquée.\n",
    "\n",
    "Le défaut, c'est que dans un projet où notre objectif est de capturer la dynamique thermique d'une zone géographique définie, entraîner notre modèle sur des données synthétiques peut devenir un obstacle à l'interprétabiltié causale et physique.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Dans l'optique de pouvoir comparer avec notre état réduit initial, nous allons réaliser une sélection de *features* (une seconde famille de méthode pour réduire un jeu de donnée). Ces méthodes permettent de conserver les variables à variance élevée (celle qui sont les plus explicatives) ou à variance conditionnelle.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 2. Théorie\n",
    "\n",
    "> #### Qu'est-ce que la régularisation et pourquoi est-elle nécessaire ?\n",
    "\n",
    "Dans les systèmes physiques à haute dimension, on observe souvent que le nombre de variables $N$ est grand, le nombre d'observations $T$ est limité et que les variables sont fortement corrélées. Ce contexte pose un problème de régression classique ($y = X\\beta + \\varepsilon$) qui mène le plus souvent à des approches erronées (solutions instables, sur-apprentissage et/ou faible capacité à généraliser). Lesquelles, dans notre cas, sont :\n",
    "\n",
    "- un cas critique avec $N \\geq T$, car la solution $\\hat{\\beta}$ n'existe pas ou instable\n",
    "- une multicolinéarité structurelle majeure et problématique, car le modèle n'arrive pas à \"choisir\" entre plusieurs variables redondantes (points spatiaux voisins fortement corrélés)\n",
    "\n",
    "La régularisation est une méthode solutionnant cette problématique. Celle-ci consiste à rajouter une contrainte (ou pénalité) supplémentaire à la régression classique dans le but de stabiliser la solution, comme suit :\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\space || y - X\\beta||²_2 + \\lambda \\mathcal{R}(\\beta)\n",
    "$$\n",
    "\n",
    "Il existe 3 principaux types de régressions classiques :\n",
    "\n",
    "- Régularisation L2 dite *Ridge* :\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(\\beta) = ||\\beta||²_2\n",
    "$$\n",
    "\n",
    "$\\to$ réduit l'amplitude des coefficients et stabilise la solution (utile pour la prédiction).\n",
    "\n",
    "- Régularisation L1 dite *LASSO* :\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(\\beta) = ||\\beta||_1 = \\underset{j}{\\sum}\\beta_j\n",
    "$$\n",
    "$\\to$ pousse certains coefficients à zéro et induit donc une sélection de variables (transformant le problème de régression en problème de sélection, notre cas).\n",
    "\n",
    "- Régularisation L1 + L2 dite *Elastic Net* :\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(\\beta) = \\alpha ||\\beta||_1 + (1-\\alpha)||\\beta||²_2\n",
    "$$\n",
    "$\\to$ compromission entre stabilité et parcimonie.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "De nombreux problèmes physiques, la fonction capturant la dynamique dépend généralement d'un petit nombre de variables où les interactions sont parcimonieuses. La régularisation L1 nous permet donc d'identifier les variables qui gouvernent tout en conservant une interprétabilité physique claire.\n",
    "\n",
    "Notre motivation derrière l'utilisation de cette combinaison d'approches est d'utiliser l'EOF pour \"comprendre\" la structure globale, mais en définissant notre état réduit uniquement dans l'espace physique. Pour plus de détail sur la PCA/EOF (voir notebook 10 et 14). La combinaison que nous allons implémenter implique d'utiliser l'opérateur de régularisation LASSO (Least Absolute Shrinkage and Selection Operator). Nous minimisons ainsi le RSS, mais augmenté de la pénalité L1 du LASSO, nous donnant : \n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\space || y - X\\beta||²_2 + \\lambda \\underset{j}{\\sum}\\beta_j\n",
    "$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implémentation d'une méthode de sélection\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Nous allons implémenter une combinaison simple (SINDy + Lasso) pour réduire notre jeu de données d'anomalies désaisonnalisées par la sélection.\n",
    "\n",
    "On commence par charger notre jeu de données post analyse statistique et importons les librairies qui nous serons utiles. On *reshape* le champ de SST en une matrice 2D en \"empilant\" les dimensions spatiales (ainsi chaque localisation correspond à un élément de la colonne spatiale).\n",
    "\n",
    "Le champ SST est initialement un champ spatio-temporel : \n",
    "\n",
    "$$\n",
    "\\text{SST}(t,\\phi,\\lambda)\n",
    "$$\n",
    "\n",
    "Et comme dans notre précédente extraction, nous le reformulons en une matrice de données :\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{T\\times N}\n",
    "$$\n",
    "\n",
    "où :\n",
    "- T = nombre d'instants temporels\n",
    "- N = nombre de point spatiaux\n",
    "\n",
    "Chaque colonne correspondra ainsi à une variable physique réelle : \n",
    "\n",
    "$$\n",
    "X(t) = [x_1(t), x_2(t), \\dots, x_N(t)]\n",
    "$$\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "ds = xr.open_dataset(\"data/processed/sstDeseasonalizedCOPERNICUS20102019.nc\")\n",
    "\n",
    "sst = ds[\"analysed_sst\"]\n",
    "\n",
    "# Shaping from (time, lat, lon) to (time, space)\n",
    "sstStacked = sst.stack(space=(\"latitude\", \"longitude\"))\n",
    "sstStacked = sstStacked.dropna(\"space\")\n",
    "\n",
    "X = sstStacked.values  # (T, Nspace)\n",
    "dt = 1.0  # time step in days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "En suite, on applique une standardisation colonne par colonne. C'est indispensable car :\n",
    "\n",
    "- PCA/EOF maximise la variance :\n",
    "$$\n",
    "\\text{max Var}(u^{\\perp}X)\n",
    "$$\n",
    "$\\to$ sans normalisation, les régions à forte variance dominent artificiellement.\n",
    "- Lasso résout (voir la partie théorie de la méthode utilisée):\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}}||y - X\\beta||²_2 + \\alpha||\\beta||_1\n",
    "$$\n",
    "$\\to$ cela correspond à une régression linéaire classique, plus un facteur dit \"pénalisant\" $\\mathcal{l}_1$ qui dépend directement de l'échelle.\n",
    "\n",
    "Par une normalisation du type :\n",
    "\n",
    "$$\n",
    "\\tilde{x}(t) = \\frac{x(t) - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "$\\to$ on garantie que la sélection repose sur la dynamique plutôt que sur l'amplitude brute.\n",
    "\n",
    "On veille tout comme dans la réduction via PCA, à performer notre standardisation à partir des données d'entraînement pour éviter tout biais d'anticipation.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = X.shape[0]\n",
    "split = int(0.8 * T)\n",
    "\n",
    "XTrain = X[:split]\n",
    "XTest   = X[split:]\n",
    "\n",
    "# We use the StandardScaler wich simply removes the mean and scales to unit variance (we fit only on the training set)\n",
    "scaler = StandardScaler()\n",
    "XTrainScaled = scaler.fit_transform(XTrain)\n",
    "XTestScaled   = scaler.transform(XTest) # we just apply the transformation because we already fitted the scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "> #### Pourquoi réutilise-t-on la PCA ?\n",
    "\n",
    "C'est le *twist*. On applique la PCA non pas pour utiliser les modes résultant comme variables d'état de notre état réduit mais plutôt comme outil intermédiaire. Elles nous servent uniquement commme élément d'observation/comparaison pour, par la suite, identifier les variables physiques du jeu qui structurent la dynamique globale (puis à les sélectionner). Pour plus de précisions sur la méthode de la PCA, voir les notebooks 10 et 14.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We perform PCA like notebook 14pcaReduction.ipynb\n",
    "\n",
    "nComponents = 150 # We choose K = 100 principal components as before    \n",
    "pca = PCA(n_components=nComponents, svd_solver=\"full\") # We choose the full SVD solver as we did before\n",
    "\n",
    "PCsTrain = pca.fit_transform(XTrainScaled) # (Ttrain, K)\n",
    "PCsTest = pca.transform(XTestScaled) # (Ttest, K)\n",
    "\n",
    "EOFs = pca.components_.T # (Nspace, K)\n",
    "explainedVar = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Dans cette étape, nous classons les variables physiques en fonction de leur contribution aux modes de variabilité dominants. L'idée est de mesurer la contribution de la variable $x_j$ au mode $k$, où chaque EOF suit :\n",
    "\n",
    "$$\n",
    "u_k = (u_{1k}, u_{2k}, \\dots, u_{Nk})\n",
    "$$\n",
    "\n",
    "Ainsi, on définit un score comme : \n",
    "\n",
    "$$\n",
    "\\mathcal{S}_j = \\overset{K}{\\underset{k=1}{\\sum}}\\lambda_k |u_{jk}|\n",
    "$$\n",
    "\n",
    "où:\n",
    "- $\\lambda_k$ = variance expliquée du mode $k$\n",
    "- $|u_{jk}|$ = coefficients de $u_k$\n",
    "\n",
    "Une variable sera décelée important si elle apparaît fortement et/ou dans les modes (énergétiquement) dominants.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are ranking based on EOF's computed on training set only\n",
    "\n",
    "topKModes = 30\n",
    "importance = np.zeros(EOFs.shape[0])\n",
    "\n",
    "for k in range(topKModes):\n",
    "    importance += explainedVar[k] * np.abs(EOFs[:, k])\n",
    "\n",
    "nCandidates = 20\n",
    "candidateIndices = np.argsort(importance)[-nCandidates:] # argsort function permits to sort and get the indices of the sorted array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "On cherche un signal directionnel pour la sélection dynamique. Pour nourrir ce but, on approxime par dérivation par différences finies centrées. De cette manière, on fait l'hypothèse implicite que le système suit une dynamique continue de nature :\n",
    "\n",
    "$$\n",
    "\\frac{dX}{dt} = F(X)\n",
    "$$\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dXdtTrain = np.gradient(XTrainScaled, dt, axis=0)\n",
    "dXdtTest  = np.gradient(XTestScaled, dt, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&nbsp;\n",
    "\n",
    "On choisit le premier PC comme cible dynamique qui sera notre critère principal de sélection. C'est lui qui capture la plus grande part de variance et donc la dynamique dominante du système. Il vérifie :\n",
    "\n",
    "$$\n",
    "a_1(t) = u_1^\\perp X(t)\n",
    "$$\n",
    "\n",
    "Puis on résoud le problème mathématique, au coeur de la méthode LASSO, suivant :\n",
    "\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}} \\left \\| \\frac{da_1}{dt} - X_c\\beta \\right \\| ²_2 + \\alpha||\\beta||_1 \n",
    "$$\n",
    "\n",
    "Pour plus d'explication de la méthode LASSO, voir la partie théorie.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose the first principal component as the target variable\n",
    "targetTrain = PCsTrain[:, 0]\n",
    "dTargetdtTrain = np.gradient(targetTrain, dt)\n",
    "\n",
    "\n",
    "XCandidatesTrain = XTrainScaled[:, candidateIndices]\n",
    "\n",
    "# We resolve dTargetdt as a sparse linear combination of the candidate predictors\n",
    "# In other words, we solve the equation dTargetdt = Lasso(XCandidates)\n",
    "lasso = Lasso(alpha=0.005, max_iter=10000)\n",
    "lasso.fit(XCandidatesTrain, dTargetdtTrain)\n",
    "\n",
    "# We select the predictors with non-zero coefficients because they are the selected variables\n",
    "selectedMask = np.abs(lasso.coef_) > 1e-6\n",
    "selectedIndices = candidateIndices[selectedMask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Résultat, nous avons :\n",
    "\n",
    "$$\n",
    "\\frac{da_1}{dt} \\approx \\underset{j \\in S}{\\sum} \\beta_j x_j\n",
    "$$\n",
    "\n",
    "On peut désormais construire notre second état réduit à partir des indices sélectionnés *a posteriori*. L'état réduit est :\n",
    "\n",
    "$$\n",
    "X_r(t) = [x_{j_1}(t), x_{j_2}(t), \\dots, x_{j_m}(t)]\n",
    "$$\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected variable indices (in original space): [3225 2902 3667 2966 3771 3773 3562 3095 3563 3666 3099 2968 3224 3222\n",
      " 2967 3098 3096]\n",
      "Lasso coefficients for selected variables: [-10.399301     0.25374225  -3.408097     9.09143      0.08342078\n",
      "   0.1791825    0.14700928  -9.73439      3.1267788   -0.05899213\n",
      "   6.475079     5.1578197    6.19796      4.5359554   -8.998721\n",
      "  -0.72967386  -1.8719403 ]\n"
     ]
    }
   ],
   "source": [
    "XTrainReduced = XTrainScaled[:, selectedIndices]\n",
    "XTestReduced  = XTestScaled[:, selectedIndices]\n",
    "\n",
    "dXdtTrainReduced = np.gradient(XTrainReduced, dt, axis=0)\n",
    "dXdtTestReduced  = np.gradient(XTestReduced, dt, axis=0)\n",
    "\n",
    "# Print non-zero coefficients and corresponding variable indices to see the scale of the selected variables\n",
    "print(\"Selected variable indices (in original space):\", selectedIndices)\n",
    "print(\"Lasso coefficients for selected variables:\", lasso.coef_[selectedMask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Propriété de notre état**\n",
    "\n",
    "- aucune combinaison linéaire\n",
    "- aucune projection\n",
    "- variables physiques uniquement\n",
    "\n",
    "Nous n'avons plus qu'à sauvegarder l'output de notre programme en fichier NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitLabel = np.array(\n",
    "    [\"train\"] * XTrainReduced.shape[0] +\n",
    "    [\"test\"]  * XTestReduced.shape[0]\n",
    ")\n",
    "\n",
    "# We build a new xarray Dataset to store the reduced state\n",
    "dsReduced = xr.Dataset(\n",
    "    {\n",
    "        \"sstReducedScaled\": (\n",
    "            (\"time\", \"space\"),\n",
    "            np.vstack([XTrainReduced, XTestReduced])\n",
    "        ),\n",
    "        \"dSstReduceddtScaled\": (\n",
    "            (\"time\", \"space\"),\n",
    "            np.vstack([dXdtTrainReduced, dXdtTestReduced])\n",
    "        ),\n",
    "        \"split\": ((\"time\",), splitLabel),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": sstStacked[\"time\"].values,\n",
    "        \"space\": selectedIndices,\n",
    "    },\n",
    "    attrs={\n",
    "        \"title\": \"Reduced SST dynamical state (COPERNICUS)\",\n",
    "        \"method\": (\n",
    "            \"EOF-based exploratory analysis followed by sparse \"\n",
    "            \"dynamic variable selection (LASSO).\"\n",
    "        ),\n",
    "        \"normalization\": (\n",
    "            \"Variables are standardized using statistics computed \"\n",
    "            \"on the training period only.\"\n",
    "        ),\n",
    "        \"time_step\": \"1 day\",\n",
    "        \"dataset_origin\": \"COPERNICUS SST (2010–2019)\",\n",
    "        \"author\": \"Your Name\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# If file already exists, it will remove it\n",
    "import os\n",
    "if os.path.exists(\"data/processed/sstReducedState2COPERNICUS20102019.nc\"):\n",
    "    os.remove(\"data/processed/sstReducedState2COPERNICUS20102019.nc\")\n",
    "\n",
    "dsReduced.to_netcdf(\"data/processed/sstReducedState2COPERNICUS20102019.nc\") # then save thanks to xarray to_netcdf function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**Structure du fichier *sstReducedState2COPERNICUS20102019.nc***\n",
    "\n",
    "- XReducedScaled → observables physiques originales et dynamiquement pertinent sélectionnés\n",
    "\n",
    "- dXdtReducedScaled → dérivée temporelle discrète des variables d'état réduit calculée par différences centrées finies\n",
    "\n",
    "- coords :\n",
    "\n",
    "    - time : axe temporel original\n",
    "\n",
    "    - space : indices des points spatiaux du couple $\\text{(latitude, longitude)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (flake)",
   "language": "python",
   "name": "flake-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
