{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&nbsp;\n",
    "\n",
    "# 14. Réduction via PCA/EOF\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 1. Rappel et objectif\n",
    "\n",
    "Ce notebook, à la différence des précédents, n'a pas de but analytique. L'objectif est de construire concrètement l’état réduit retenu et le sauvegarder dans un format standardisé pour les étapes ultérieures.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "> #### Quel est l'état à retenir ?\n",
    "\n",
    "Rappelons en condensé ce qui a été énoncé dans le notebook 10.\n",
    "\n",
    "En l'état, nous démarrons le pré-traitement avec un champs spatial de SST qui évolue dans le temps avec des milliers de points spatiaux corrélés entre eux. \n",
    "\n",
    "La nature géophysique des champs avec lesquels nous travaillons nous avait amené à penser que l'EOF serait la méthode adaptée pour réduire notre série d'anomalies de SST désaisonnalisée. On transforme ainsi notre matrice actuelle de la manière suivante :\n",
    "\n",
    "$$ X(x, y, t) \\approx \\overset{K}{\\underset{k=1}{\\sum}}a_k(t)\\phi_k(x,y) $$\n",
    "\n",
    "La réduction de dimensionnalité est nécessaire pour clarifier ce qu'on veut faire apprendre à notre futur modèle. Je cite la partie 1. du notebook 10 : \"Sans cela on s'expose à un apprentissage instable, une sur-paramétrisation massive et une interprétation impossible. Le modèle ne doit apprendre ni le bruit, ni la grille, mais la dynamique.\". Suivant l'EOF, on obtient donc un vecteur d'état composait des PCs (principaux composants) de la réduction, soit :\n",
    "\n",
    "$$\n",
    "\\dot{a}_k(t) = f(\\begin{pmatrix} a_1(t) \\\\ a_2(t) \\\\ \\vdots \\\\ a_k(t) \\end{pmatrix})\n",
    "$$\n",
    "\n",
    "Finalement, on fournit l'état dynamique réduit suivant au modèle SciML :\n",
    "\n",
    "$$\n",
    "\\dot{a}_k(t) = \\begin{pmatrix} \\dot{a}_1(t) \\\\ \\dot{a}_2(t) \\\\ \\vdots \\\\ \\dot{a}_1(t) \\end{pmatrix} = f_\\theta(a_1(t), a_2(t),..., a_k(t))\n",
    "$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 2. Construction de l’état réduit par EOF\n",
    "\n",
    "Nous avons déjà réalisé les vérifications et discuter de la théorie derrière la méthode de réduction de dimensionnalité PCA/EOF (voir Notebook 10).\n",
    "\n",
    "Appliquons réellement la méthode au jeu de donnée, avec les paramètres choisies préalablement (~100 modes, SingularValueDecomposition Solver en mode \"full\", EOF) et sauvegardons le résultat dans notre répertoire *data/processed/*.\n",
    "\n",
    "Voir le lien suivant pour plus de précisions sur la fonction *PCA* de *sklearn.decomposition* utilisée.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
    "\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 12MB\n",
      "Dimensions:       (time: 3652, mode: 150, latitude: 60, longitude: 140)\n",
      "Coordinates:\n",
      "  * time          (time) datetime64[ns] 29kB 2010-01-01 ... 2019-12-31\n",
      "  * mode          (mode) int64 1kB 0 1 2 3 4 5 6 ... 143 144 145 146 147 148 149\n",
      "  * latitude      (latitude) float32 240B 48.03 48.08 48.13 ... 50.93 50.98\n",
      "  * longitude     (longitude) float32 560B -4.975 -4.925 -4.875 ... 1.925 1.975\n",
      "    month         (time) int64 29kB ...\n",
      "Data variables:\n",
      "    PCs           (time, mode) float32 2MB -13.05 -1.123 ... 0.5583 -0.3348\n",
      "    EOFs          (mode, latitude, longitude) float64 10MB 0.01074 ... 0.03121\n",
      "    explainedVar  (mode) float32 600B 0.8209 0.04989 ... 5.848e-05 5.835e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load deseasonalized SST data\n",
    "ds = xr.open_dataset(\"data/processed/sstDeseasonalizedCOPERNICUS20102019.nc\")\n",
    "\n",
    "# Extract SST data and reshape for PCA\n",
    "sst = ds[\"analysed_sst\"]\n",
    "sstStacked = sst.stack(space=(\"latitude\", \"longitude\"))\n",
    "sstStacked = sstStacked.dropna(\"space\")\n",
    "\n",
    "X = sstStacked.values  # shape (Nt, Nspace)\n",
    "\n",
    "K = 150  # Number of principal components to retain plus some margin if we want to test more later\n",
    "\n",
    "pca = PCA(n_components=K, svd_solver=\"full\") # We apply PCA with K components and full SVD solver so we respect timeStep/numberSpatialPoints ratio\n",
    "PCs = pca.fit_transform(X)        # a_k(t)\n",
    "EOFs = pca.components_            # phi_k(x)\n",
    "explainedVar = pca.explained_variance_ratio_\n",
    "\n",
    "# Next step, we save PCs and EOFs in an appropriate formated xarray Dataset\n",
    "\n",
    "# We reconstruct EOFs into original spatial shape with NaNs where data was missing\n",
    "template = sst.isel(time=0)\n",
    "\n",
    "# Create DataArray for EOFs with NaNs in missing data locations\n",
    "EOFsDa = xr.DataArray(\n",
    "    np.full((K , template.sizes[\"latitude\"], template.sizes[\"longitude\"]), np.nan), # K, lat, lon as shape filled with NaNs\n",
    "    dims=(\"mode\", \"latitude\", \"longitude\"), # dimension names in the CDF\n",
    "    coords={\n",
    "        \"mode\": np.arange(K), # filling with mode numbers\n",
    "        \"latitude\": template[\"latitude\"], # filling with latitude values\n",
    "        \"longitude\": template[\"longitude\"], # filling with longitude values\n",
    "    },\n",
    ")\n",
    "\n",
    "EOFsDa.values.reshape(K, -1)[:, ~np.isnan(sst.isel(time=0).values.flatten())] = EOFs # This line fills the EOFsDa with EOF values at non-NaN locations\n",
    "\n",
    "# Then we just form a Dataset with PCs, EOFs and explained variance\n",
    "dsOut = xr.Dataset(\n",
    "    {\n",
    "        \"PCs\": ((\"time\", \"mode\"), PCs),\n",
    "        \"EOFs\": EOFsDa,\n",
    "        \"explainedVar\": ((\"mode\",), explainedVar),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": sst[\"time\"],\n",
    "        \"mode\": np.arange(K),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Finally we save the Dataset to a netCDF file\n",
    "dsOut.to_netcdf(\"data/processed/sstReducedStateCOPERNICUS20102019.nc\")\n",
    "\n",
    "print(dsOut) # See magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "On obtient finalement la structure de fichier suivante comme entrée du SciML.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Structure du fichier *sstReducedStateCOPERNICUS20102019.nc***\n",
    "\n",
    "- PCs(time, mode) → coefficients temporels \n",
    "\n",
    "- EOFs(mode, latitude, longitude) → modes spatiaux associés aux PCs\n",
    "\n",
    "- explainedVar(mode) → variance expliquée par chaque mode\n",
    "\n",
    "- coords :\n",
    "\n",
    "    - time : axe temporel original\n",
    "\n",
    "    - latitude, longitude : grille spatiale\n",
    "\n",
    "    - mode : index des composantes EOF\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 3. Alternatives de réduction\n",
    "\n",
    "#### Later\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (flake)",
   "language": "python",
   "name": "flake-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
