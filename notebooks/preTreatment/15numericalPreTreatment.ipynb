{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "# 15. Pré-traitement numérique pour SciML\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Ce notebook vise à préparer l’état réduit issu de l’analyse EOF afin qu’il soit compatible avec l’apprentissage de modèles dynamiques SciML, cela en garantissant la stabilité numérique, l’absence de fuite d’information et la cohérence temporelle. On peut le réduire à une question centrale.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "> #### Est-ce mon état réduit est numériquement sain pour apprendre une dynamique ?\n",
    "\n",
    "L’état réduit est constitué des $K$ premiers coefficients EOF (~150 modes dans le NetCDF), stockés sous la forme d’une série temporelle multivariée.\n",
    "\n",
    "Ce que l'on connaît des modèles dynamique SciML, c'est leur tendance à être sensible aux échelles. Nous l'apprenons de cette source Application of Reduced-Order Models for Temporal Multiscale Representations in the Prediction of Dynamical Systems (https://arxiv.org/html/2510.18925v1) après une recherche avec le mot-clé \"Model Reduction\".\n",
    "\n",
    "Globalement, ce que l'on en retire, c'est que ce type de modèle est conçu pour apprendre une loi d'évolution temporelle. Si l'état réduit \"contient\" des composantes associées à diverses échelles temporelles, le modèle appliqué doit réussir à différencier ces échelles au risque de sur-lisser ou ne modèliser qu'une seule échelle.\n",
    "\n",
    "Dans un système réel (comme la SST), il existe des interactions entre échelles lentes et rapides (ou un spectre de K échelles). Les approches dites \"naïves\" peinent à capturer ces comportements qui opèrent simultanément sur le même jeu de données. \n",
    "\n",
    "Chaque mode (ou degré de liberté) peut donc évoluer à son propre rythme, et dans un système tel à plusieurs échelles, il nous faut au plus simplifier ce travail de différenciation pour que la formulation proposée par la modélisation dynamique SciML reproduise au plus fidèlement les signatures lentes/rapides et leurs couplages(ou le fait que l'évolution d’une composante du système dépend de l’état d’une ou plusieurs autres composantes). Exemple : un gradient thermique large échelle (EOF 1) peut conditionner l’intensité des fronts côtiers (EOF 3).\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 2. Normalisation et mise à l'échelle\n",
    "\n",
    "> #### Pourquoi faire cela ?\n",
    "\n",
    "La normalisation et la mise à l'échelle sont une réponse simple et efficace au problème de différencition d'échelles exprimée ci-dessus. On veut éviter qu'un mode ne domine trop numériquement. Mathématiquement cela se traduit par une normalisation par l'écart-type (ou scaling), comme suit :\n",
    "\n",
    "$$\n",
    "\\tilde{a}_k(t) = \\frac{a_k(t)}{\\sigma_k}\n",
    "$$\n",
    "où :\n",
    "- $\\tilde{a}_k(t)$ : état réduit $k$ normalisé de sortie\n",
    "- $a_k(t)$ : état réduit $k$ d'entrée \n",
    "- $\\sigma_k$ : écart-type des PCs\n",
    "\n",
    "En effet, diviser par l'écart-type ajuste notre état réduit de manière à affaiblir les PCs qui dominent et renforcer ceux qui sont dominés.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "dsReducedState = xr.open_dataset(\"data/processed/sstReducedStateCOPERNICUS20102019.nc\")\n",
    "\n",
    "PCs = dsReducedState[\"PCs\"].values\n",
    "\n",
    "time = dsReducedState[\"time\"] # Time splitting\n",
    "\n",
    "trainMask = time < np.datetime64(\"2018-01-01\") # We constrain training data to pre-2018\n",
    "\n",
    "PCsTrain = dsReducedState[\"PCs\"].sel(time=trainMask).values\n",
    "PCsVal   = dsReducedState[\"PCs\"].sel(time=~trainMask).values\n",
    "\n",
    "stdTrain  = PCsTrain.std(axis=0)\n",
    "\n",
    "PCsTrainScaled = PCsTrain / stdTrain\n",
    "PCsValScaled   = PCsVal / stdTrain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "On revérifie que le centrage est correct pour garantir l'absence de biais constant dans la dynamique que l'on souhaite apprendre. On constate la moyenne de chaque PC est proche de 0 (de l'ordre du dix-millième au cent-milliardième selon le PC). \n",
    "\n",
    "C'est cette vérification qui justifie que nous n'ayons pas besoin de soustraire la moyenneau numérateur de la formule de normalisation énoncée.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "> #### Pourquoi normaliser uniquement sur les données d'entraînement ?\n",
    "\n",
    "On sait que notre état réduit dépend désormais uniquement du temps, car on travaille bien avec $a_k(t)$.\n",
    "\n",
    "On a expliqué pourquoi nous voulions normaliser les PCs, mais si nous calculons l'écart-type avec les PCs pour $t$ allant de 2010 à 2019 et que par la suite nous entrainons notre modèle sur une part des données (disons 01/01/2018). Un biais s'immisce dans notre apprentissage car la normalisation est réalisé avec une valeur qui incorpore une part de l'information future (l'écart-type). \n",
    "\n",
    "On appelle cela le biais d'anticipation, c'est-à-dire entraîner un modèle avec des paramètres qui capture une part de l'explicabilité future des valeurs que l'on cherche à prédire. L'apprentissage d'un modèle avec un tel biais aura tendance à sur-évaluer la précision de prédiction réelle du modèle à l'évaluation sur les données de tests. Dans ce cas on prédirait \"mieux\" les anomalies de SST de 2018-2019 mais moins bien que l'on le pourrait pour les années qui suivent.\n",
    "\n",
    "Cela nous oblige dès maintenant à traiter la question du *splitting* des données d'entraînement et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0034608492\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# We recheck if scaling worked as intended\n",
    "print(np.mean(PCsTrainScaled.mean(axis=0))) # ≈ 0\n",
    "print(np.mean(PCsTrainScaled.std(axis=0))) # ≈ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous nous assurons toujours que ces mesures restent proches des valeurs de référence voulues sous peine d'une exposition à des instabilités ou d'apprendre une tendance parasite lors de l'entraînement du modèle dynamique SciML.\n",
    "\n",
    "On rappelle également que le pas de temps est régulier (voir Notebook 02), c'est important dans la mesure où les modèles SciML supposent une dynamique bien définie concernant la discrétisation temporelle (un pas de temps régulier pour garantir la validité de la formulation dynamique).\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## 3. Construction du conteneur avant SciML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeTrain = time[trainMask]\n",
    "timeVal   = time[~trainMask]\n",
    "\n",
    "dt = float((timeTrain[1] - timeTrain[0]) / np.timedelta64(1, \"D\")) # We precise float to avoid conflict between xarray and numpy types\n",
    "\n",
    "tTrain = (np.arange(len(timeTrain)) * dt).astype(np.float32)\n",
    "tVal   = (np.arange(len(timeVal)) * dt).astype(np.float32)\n",
    "\n",
    "dataPrepared = {\n",
    "    \"PCsTrain\": PCsTrainScaled,\n",
    "    \"PCsVal\": PCsValScaled,\n",
    "    \"tTrain\": tTrain,\n",
    "    \"tVal\": tVal,\n",
    "    \"std\": stdTrain,\n",
    "}\n",
    "\n",
    "# Saving prepared data\n",
    "np.savez(\"data/processed/sstReducedStateCOPERNICUS20102019Prepared.npz\", **dataPrepared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python m1Project",
   "language": "python",
   "name": "m1project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
