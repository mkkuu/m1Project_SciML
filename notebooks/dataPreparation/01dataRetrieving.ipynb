{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 01 - Retrieving raw data\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "How should we select wich data feed is relevant ? That is the question we firstly asked ourselves. \n",
    "\n",
    "As the specifications notice it, our final goal is to \"understand and simulate the evolution of the SST parameter in time, study the stability of the model and evaluate its capacity to reproduce or predict observed fluctuations\". The success of this mission therefore depends first and foremost on the quality of the data on which we base our work. Based on this reasoning, we decided to download and compare two similar products from the following feeds: NOAA and Copernicus Marine.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "> ### How did we select the following products?\n",
    "\n",
    "First, we focused on Level 4 (L4) products. The term “Lx” refers to the processing level of a dataset. L4 corresponds to a highly processed data product, in which multiple observation sources are combined, and optimal interpolation techniques are applied to fill data gaps (OI marker in the name of the product for NOAA). Additional corrections and quality controls may also be included.\n",
    "\n",
    "As a result, L4 products provide spatially and temporally complete gridded fields, with no missing values and reduced inconsistencies. Since our objective is to model, predict, and quantitatively compare ocean surface temperature datasets, we consider L4 products to be the most appropriate choice for this study.\n",
    "\n",
    "Last, we mainly focus on SST parameter(Sea Surface Temperature) for the moment, so we select SST marked product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# test loading a part of COPERNICUS dataset using xarray\n",
    "\n",
    "\n",
    "ds = xr.open_dataset(\"data/raw/unmerged/C3S-GLO-SST-L4-REP-OBS-SST_1767389520200_part1.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually downloaded NetCDF database on both official website of NOAA and Copernicus Marine. \n",
    "\n",
    "Concerning NOAA, we had to download the whole world database year by year, hence, we will need to merge those 10 files and only keep the data that interests us (Manche area, latitude between 51 and 48, longitude between 5 and -2). \n",
    "\n",
    "Concerning Copernicus Marine, website permits us to download only the data we need by applying spatial and time constraint, we will only need to merge 2 NetCDF files (since download from website is size restricted, we had to download it in 2 times, 2010-2015 and 2015-2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# We convert to NetCDF\u001b[39;00m\n\u001b[32m     40\u001b[39m dsNOAA.to_netcdf(\u001b[33m\"\u001b[39m\u001b[33mdata/raw/merged/sstNOAA20102019.nc\u001b[39m\u001b[33m\"\u001b[39m, encoding={\u001b[33m\"\u001b[39m\u001b[33msst\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mzlib\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcomplevel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m}}) \u001b[38;5;66;03m# We convert NetCDF NOAA file merged to a merged nc file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mdsCOPERNICUS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/raw/merged/sstCOPERNICUS20102019.nc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalysed_sst\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzlib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcomplevel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# We convert NetCDF COPERNICUS file merged to a merged nc file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/pgfi6na9nwyz27ibbl7v84smw66bsvkm-python3-3.12.12-env/lib/python3.12/site-packages/xarray/core/dataset.py:2110\u001b[39m, in \u001b[36mDataset.to_netcdf\u001b[39m\u001b[34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\u001b[39m\n\u001b[32m   2107\u001b[39m     encoding = {}\n\u001b[32m   2108\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwriters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[32m-> \u001b[39m\u001b[32m2110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[32m   2111\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/pgfi6na9nwyz27ibbl7v84smw66bsvkm-python3-3.12.12-env/lib/python3.12/site-packages/xarray/backends/writers.py:455\u001b[39m, in \u001b[36mto_netcdf\u001b[39m\u001b[34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multifile \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m autoclose:  \u001b[38;5;66;03m# type: ignore[redundant-expr,unused-ignore]\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m         \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    457\u001b[39m         store.sync()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/pgfi6na9nwyz27ibbl7v84smw66bsvkm-python3-3.12.12-env/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:673\u001b[39m, in \u001b[36mNetCDF4DataStore.close\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/pgfi6na9nwyz27ibbl7v84smw66bsvkm-python3-3.12.12-env/lib/python3.12/site-packages/xarray/backends/file_manager.py:242\u001b[39m, in \u001b[36mCachingFileManager.close\u001b[39m\u001b[34m(self, needs_lock)\u001b[39m\n\u001b[32m    240\u001b[39m file = \u001b[38;5;28mself\u001b[39m._cache.pop(\u001b[38;5;28mself\u001b[39m._key, default)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# We make sure Dask manager is set up or the download will fail\n",
    "\n",
    "# Here we are instancing a list of names of the files we want to merge\n",
    "\n",
    "filesNOAA = sorted([\n",
    "    \"data/raw/unmerged/sst.day.mean.2010_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2011_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2012_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2013_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2014_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2015_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2016_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2017_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2018_NOAA.nc\",\n",
    "    \"data/raw/unmerged/sst.day.mean.2019_NOAA.nc\",\n",
    "])\n",
    "\n",
    "filesCOPERNICUS = sorted([\n",
    "    \"data/raw/unmerged/C3S-GLO-SST-L4-REP-OBS-SST_1767389520200_part1.nc\",\n",
    "    \"data/raw/unmerged/C3S-GLO-SST-L4-REP-OBS-SST_1767389601469_part2.nc\"\n",
    "])\n",
    "\n",
    "# We use xarray lib to merge all nc files\n",
    "\n",
    "dsNOAA = xr.open_mfdataset(\n",
    "    filesNOAA, # specify the list of files we want to merge\n",
    "    chunks={\"time\": 365} # specify chunks size to optimize time and space allocation\n",
    ")\n",
    "\n",
    "dsCOPERNICUS = xr.open_mfdataset(\n",
    "    filesCOPERNICUS, # specify the list of files we want to merge\n",
    "    chunks={\"time\":365} # specify chunks size to optimize time and space allocation\n",
    ")\n",
    "\n",
    "dsNOAA = dsNOAA.sel(lat=slice(-2,5)) # We select the region of interest : Manche Bay\n",
    "dsNOAA = dsNOAA.sel(lon=slice(48,51)) \n",
    "\n",
    "# We convert to NetCDF\n",
    "\n",
    "dsNOAA.to_netcdf(\"data/raw/merged/sstNOAA20102019.nc\", encoding={\"sst\": {\"zlib\": True, \"complevel\": 4}}) # We convert NetCDF NOAA file merged to a merged nc file\n",
    "dsCOPERNICUS.to_netcdf(\"data/raw/merged/sstCOPERNICUS20102019.nc\", encoding={\"analysed_sst\": {\"zlib\": True, \"complevel\": 4}}) # We convert NetCDF COPERNICUS file merged to a merged nc file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (flake)",
   "language": "python",
   "name": "flake-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
