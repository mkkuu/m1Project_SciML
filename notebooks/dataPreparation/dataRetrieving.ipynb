{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving raw data\n",
    "\n",
    "How should we select wich data feed is relevant ? That is the question we firstly asked ourselves. As the specifications notice it, our final goal is to \"understand and simulate the evolution of the SST parameter in time, study the stability of the model and evaluate its capacity to reproduce or predict observed fluctuations\". The success of this mission therefore depends first and foremost on the quality of the data on which we base our work. Based on this reasoning, we decided to download and compare two similar products from the following feeds: NOAA and Copernicus Marine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did we select the following products?\n",
    "\n",
    "First, we focused on Level 4 (L4) products. The term “Lx” refers to the processing level of a dataset. L4 corresponds to a highly processed data product, in which multiple observation sources are combined, and optimal interpolation techniques are applied to fill data gaps (OI marker in the name of the product for NOAA). Additional corrections and quality controls may also be included.\n",
    "\n",
    "As a result, L4 products provide spatially and temporally complete gridded fields, with no missing values and reduced inconsistencies. Since our objective is to model, predict, and quantitatively compare ocean surface temperature datasets, we consider L4 products to be the most appropriate choice for this study.\n",
    "\n",
    "Last, we mainly focus on SST parameter(Sea Surface Temperature) for the moment, so we select SST marked product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 430MB\n",
      "Dimensions:                (time: 1827, latitude: 60, longitude: 140)\n",
      "Coordinates:\n",
      "  * time                   (time) datetime64[ns] 15kB 2010-01-01 ... 2015-01-01\n",
      "  * latitude               (latitude) float32 240B 48.03 48.08 ... 50.93 50.98\n",
      "  * longitude              (longitude) float32 560B -4.975 -4.925 ... 1.975\n",
      "Data variables:\n",
      "    analysed_sst           (time, latitude, longitude) float32 61MB ...\n",
      "    analysed_st            (time, latitude, longitude) float32 61MB ...\n",
      "    analysis_error_sst     (time, latitude, longitude) float32 61MB ...\n",
      "    analysis_error_st      (time, latitude, longitude) float32 61MB ...\n",
      "    mask                   (time, latitude, longitude) float32 61MB ...\n",
      "    sea_ice_fraction       (time, latitude, longitude) float32 61MB ...\n",
      "    sea_ice_fraction_flag  (time, latitude, longitude) float32 61MB ...\n",
      "Attributes:\n",
      "    Conventions:       CF-1.11\n",
      "    title:             Global Sea and Ice Surface Temperature, L4, 5km daily ...\n",
      "    institution:       Danish Meteorological Institute, DMI\n",
      "    source:            ESA SST CCI v3.0 and C3S v3.0 L3U, AASTIv2 IST L2P. OS...\n",
      "    history:           Version 1.0\n",
      "    references:        Høyer, J. L. and She, J., Optimal interpolation of sea...\n",
      "    comment:           IN NO EVENT SHALL DMI OR ITS REPRESENTATIVES BE LIABLE...\n",
      "    subset:source:     ARCO data downloaded from the Marine Data Store using ...\n",
      "    subset:productId:  SST_GLO_SST_L4_REP_OBSERVATIONS_010_024\n",
      "    subset:datasetId:  C3S-GLO-SST-L4-REP-OBS-SST_202506\n",
      "    subset:date:       2026-01-02T21:32:00.200Z\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# chargement via NedCDF (xarray lib)\n",
    "\n",
    "ds = xr.open_dataset(\"data/raw/C3S-GLO-SST-L4-REP-OBS-SST_1767389520200_part1.nc\")\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually downloaded NetCDF database on both official website of NOAA and Copernicus Marine. Concerning NOAA, we had to download the whole world database year by year, hence, we will need to merge those 10 files and only keep the data that interests us (Manche area, latitude between 51 and 48, longitude between 5 and -2). Concerning Copernicus Marine, website permits us to download only the data we need by applying spatial and time constraint, we will only need to merge 2 NetCDF files (since download from website is size restricted, we had to download it in 2 times, 2010-2015 and 2015-2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make sure Dask manager is set up or the download will fail\n",
    "\n",
    "# Here we are instancing a list of names of the files we want to merge\n",
    "\n",
    "filesNOAA = sorted([\n",
    "    \"data/raw/sst.day.mean.2010_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2011_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2012_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2013_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2014_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2015_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2016_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2017_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2018_NOAA.nc\",\n",
    "    \"data/raw/sst.day.mean.2019_NOAA.nc\",\n",
    "])\n",
    "\n",
    "filesCOPERNICUS = sorted([\n",
    "    \"data/raw/C3S-GLO-SST-L4-REP-OBS-SST_1767389520200_part1.nc\",\n",
    "    \"data/raw/C3S-GLO-SST-L4-REP-OBS-SST_1767389601469_part2.nc\"\n",
    "])\n",
    "\n",
    "# We use xarray lib to merge all nc files\n",
    "\n",
    "dsNOAA = xr.open_mfdataset(\n",
    "    filesNOAA, # specify the list of files we want to merge\n",
    "    chunks={\"time\": 365} # specify chunks size to optimize time and space allocation\n",
    ")\n",
    "\n",
    "dsCOPERNICUS = xr.open_mfdataset(\n",
    "    filesCOPERNICUS, # specify the list of files we want to merge\n",
    "    chunks={\"time\":365} # specify chunks size to optimize time and space allocation\n",
    ")\n",
    "\n",
    "# To NetCDF\n",
    "\n",
    "dsNOAA.to_netcdf(\"data/raw/sstNOAA20102019.nc\", encoding={\"sst\": {\"zlib\": True, \"complevel\": 4}}) # We convert NetCDF NOAA file merged to a merged nc file\n",
    "dsCOPERNICUS.to_netcdf(\"data/raw/sstCOPERNICUS20102019.nc\", encoding={\"analysed_sst\": {\"zlib\": True, \"complevel\": 4}}) # We convert NetCDF COPERNICUS file merged to a merged nc file\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
