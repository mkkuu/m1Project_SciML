{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-temporal coherence\n",
    "\n",
    "Nous souhaitons nous assurer que les données décrivent un même phénomène physique sur une grille spatiale et temporelle cohérente, et cela, en maintenant une continuité et une exploitabilité pour l’analyse et la modélisation.\n",
    "\n",
    "## 1. Cohérence temporelle\n",
    "\n",
    "### 1.1 Vérifier la structure du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 3652)> Size: 29kB\n",
      "array(['2010-01-01T00:00:00.000000000', '2010-01-02T00:00:00.000000000',\n",
      "       '2010-01-03T00:00:00.000000000', ..., '2019-12-29T00:00:00.000000000',\n",
      "       '2019-12-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000'],\n",
      "      shape=(3652,), dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 29kB 2010-01-01 2010-01-02 ... 2019-12-31\n",
      "Attributes:\n",
      "    long_name:     Time\n",
      "    delta_t:       0000-00-01 00:00:00\n",
      "    avg_period:    0000-00-01 00:00:00\n",
      "    axis:          T\n",
      "    actual_range:  [76701. 77065.]\n",
      "{'dtype': dtype('float64'), 'zlib': False, 'szip': False, 'zstd': False, 'bzip2': False, 'blosc': False, 'shuffle': False, 'complevel': 0, 'fletcher32': False, 'contiguous': True, 'chunksizes': None, 'source': '/home/mkkuu/Documents/GitHub/m1Project_SciML/data/raw/merged/sstNOAA20102019.nc', 'original_shape': (3652,), '_FillValue': np.float64(nan), 'units': 'days since 1800-01-01', 'calendar': 'proleptic_gregorian'}\n",
      "\n",
      "***************************************\n",
      "\n",
      "<xarray.DataArray 'time' (time: 3654)> Size: 29kB\n",
      "array(['2010-01-01T00:00:00.000000000', '2010-01-02T00:00:00.000000000',\n",
      "       '2010-01-03T00:00:00.000000000', ..., '2019-12-30T00:00:00.000000000',\n",
      "       '2019-12-31T00:00:00.000000000', '2020-01-01T00:00:00.000000000'],\n",
      "      shape=(3654,), dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 29kB 2010-01-01 2010-01-02 ... 2020-01-01\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      Time\n",
      "    axis:           T\n",
      "{'dtype': dtype('float64'), 'zlib': False, 'szip': False, 'zstd': False, 'bzip2': False, 'blosc': False, 'shuffle': False, 'complevel': 0, 'fletcher32': False, 'contiguous': True, 'chunksizes': None, 'source': '/home/mkkuu/Documents/GitHub/m1Project_SciML/data/raw/merged/sstCOPERNICUS20102019.nc', 'original_shape': (3654,), '_FillValue': np.float64(nan), 'units': 'seconds since 1970-01-01', 'calendar': 'gregorian'}\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "dsNOAA = xr.open_dataset(\"data/raw/merged/sstNOAA20102019.nc\")\n",
    "dsCOPERNICUS = xr.open_dataset(\"data/raw/merged/sstCOPERNICUS20102019.nc\")\n",
    "\n",
    "print(dsNOAA.time)\n",
    "print(dsNOAA.time.encoding)\n",
    "\n",
    "print(\"\\n***************************************\\n\")\n",
    "\n",
    "print(dsCOPERNICUS.time)\n",
    "print(dsCOPERNICUS.time.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparant les axes temporels des deux jeux de données, on constate un léger décalage de borne temporelle. Copernicus incluant un jour supplémentaire au-delà de l'année 2019. Pour garantir la cohérence temporelle stricte entre les sources, nous allons simplement restreindre le jeu de donnée COPERNICUS à la période commune 2010-2019.\n",
    "\n",
    "On note également une différence de standard de calendrier, NOAA utilisant le calendrier grégorien proleptique et COPERNICUS le grégorien simple. Cela ne fait aucune différence pour les années suivant 1582 d'après nos recherches. Aucun traitement ne sera réalisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "<xarray.DataArray 'time' (time: 3653)> Size: 29kB\n",
      "array(['2010-01-01T00:00:00.000000000', '2010-01-02T00:00:00.000000000',\n",
      "       '2010-01-03T00:00:00.000000000', ..., '2019-12-29T00:00:00.000000000',\n",
      "       '2019-12-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000'],\n",
      "      shape=(3653,), dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 29kB 2010-01-01 2010-01-02 ... 2019-12-31\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      Time\n",
      "    axis:           T\n"
     ]
    }
   ],
   "source": [
    "print(dsNOAA.time.equals(dsCOPERNICUS.time)) # False, confirmant l'inégalité des bornes temporelles\n",
    "\n",
    "dsCOPERNICUS = dsCOPERNICUS.sel(time=slice(\"2010-01-01\", \"2019-12-31\")) # We apply slicing to ensure the same time range\n",
    "\n",
    "print(dsNOAA.time.equals(dsCOPERNICUS.time)) # We check again after slicing\n",
    "\n",
    "print(dsCOPERNICUS.time) # We print the time coordinate after slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Continuité temporelle\n",
    "\n",
    "On constate toujours une inégalité entre le nombre de points temporels dans les deux jeux de données. Il semblerait qu'il y ait toujours un point temporel supplémentaire (nous voulons avoir 3652 points correspondant à 8 années à 365 jours et 2 années bissextiles à 366 jours, soit 3652 jours). Explorons l'hypothèse d'un éventuel doublon en vérifiant le pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimedeltaArray>\n",
      "['1 days']\n",
      "Length: 1, dtype: timedelta64[ns] <TimedeltaArray>\n",
      "['1 days', '0 days']\n",
      "Length: 2, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dtNOAA = pd.to_datetime(dsNOAA.time.values).to_series().diff().dropna().unique()\n",
    "dtCOPERNICUS  = pd.to_datetime(dsCOPERNICUS.time.values).to_series().diff().dropna().unique()\n",
    "\n",
    "# For each dataset :\n",
    "#   - convert time values to pandas datetime\n",
    "#   - convert it to a pandas Series to manipulate it easily\n",
    "#   - compute the difference between each time value and the previous one\n",
    "#   - drop the NaT value resulting from the diff operation\n",
    "#   - get the unique values of the resulting time differences\n",
    "\n",
    "print(dtNOAA, dtCOPERNICUS) # Print delta times for both datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultant de notre test, on constate bel et bien la présence d'un doublon dans le jeu de donnée COPERNICUS. Pourquoi ? Car on remarque la présence de l'élément '0 days' dans la liste en retour de notre opération, signifiant qu'il existe 2 points SST pour un jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01   0 days\n",
      "dtype: timedelta64[ns]\n",
      "DatetimeIndex(['2015-01-01'], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "time = pd.to_datetime(dsCOPERNICUS.time.values) # We instanciate a pandas datetime object from the time values of the COPERNICUS dataset\n",
    "dt = time.to_series().diff() # We convert it to a pandas Series and compute the time differences between each value and the previous one\n",
    "\n",
    "duplicates = dt[dt == pd.Timedelta(0)] # We filter the delta times to get only the duplicates of 0 timedelta\n",
    "print(duplicates) \n",
    "\n",
    "idx = duplicates.index # We get the index corresponding to the duplicate time values so it returns us the days where there are duplicates\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jour présent 2 fois est donc le 01 janvier 2015. En effet, ayant télécharger manuellement depuis le site officiel sur les bornes 01/01/2010-01/01/2015 puis dans un second temps 01/01/2015-01/01/2020, cela a généré un doublon à la fusion des deux fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "index = np.unique(dsCOPERNICUS.time, return_index=True)[1] # We get the unique time values and their corresponding indices\n",
    "dsCOPERNICUS = dsCOPERNICUS.isel(time=index) # We select only the unique time values using their indices\n",
    "\n",
    "print(dsCOPERNICUS.time.equals(dsNOAA.time)) # We test the equality of the time bounds again after removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comparaison sur les axes temporelles des 2 jeux de données retourne True. Nous présumons ainsi la cohérence temporelle vérifiée et les fichiers bien alignés temporellement.\n",
    "\n",
    "## 2. Cohérence spatiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'time': 3652, 'lat': 28, 'lon': 12})\n",
      "<xarray.DataArray 'lat' ()> Size: 4B\n",
      "array(-1.875, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Latitude\n",
      "    standard_name:  latitude\n",
      "    units:          degrees_north\n",
      "    actual_range:   [-89.875  89.875]\n",
      "    axis:           Y <xarray.DataArray 'lat' ()> Size: 4B\n",
      "array(4.875, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Latitude\n",
      "    standard_name:  latitude\n",
      "    units:          degrees_north\n",
      "    actual_range:   [-89.875  89.875]\n",
      "    axis:           Y\n",
      "<xarray.DataArray 'lon' ()> Size: 4B\n",
      "array(48.125, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Longitude\n",
      "    standard_name:  longitude\n",
      "    units:          degrees_east\n",
      "    actual_range:   [1.25000e-01 3.59875e+02]\n",
      "    axis:           X <xarray.DataArray 'lon' ()> Size: 4B\n",
      "array(50.875, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Longitude\n",
      "    standard_name:  longitude\n",
      "    units:          degrees_east\n",
      "    actual_range:   [1.25000e-01 3.59875e+02]\n",
      "    axis:           X\n",
      "\n",
      "***************************************\n",
      "\n",
      "FrozenMappingWarningOnValuesAccess({'time': 3652, 'latitude': 60, 'longitude': 140})\n",
      "<xarray.DataArray 'latitude' ()> Size: 4B\n",
      "array(48.02501, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  latitude\n",
      "    long_name:      Latitude\n",
      "    units:          degrees_north\n",
      "    unit_long:      Degrees North\n",
      "    axis:           Y <xarray.DataArray 'latitude' ()> Size: 4B\n",
      "array(50.97501, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  latitude\n",
      "    long_name:      Latitude\n",
      "    units:          degrees_north\n",
      "    unit_long:      Degrees North\n",
      "    axis:           Y\n",
      "<xarray.DataArray 'longitude' ()> Size: 4B\n",
      "array(-4.9750004, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  longitude\n",
      "    long_name:      Longitude\n",
      "    units:          degrees_east\n",
      "    unit_long:      Degrees East\n",
      "    axis:           X <xarray.DataArray 'longitude' ()> Size: 4B\n",
      "array(1.975, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  longitude\n",
      "    long_name:      Longitude\n",
      "    units:          degrees_east\n",
      "    unit_long:      Degrees East\n",
      "    axis:           X\n"
     ]
    }
   ],
   "source": [
    "print(dsNOAA.dims)\n",
    "print(dsNOAA.lat.min(), dsNOAA.lat.max())\n",
    "print(dsNOAA.lon.min(), dsNOAA.lon.max())\n",
    "\n",
    "print(\"\\n***************************************\\n\")\n",
    "\n",
    "print(dsCOPERNICUS.dims)\n",
    "print(dsCOPERNICUS.latitude.min(), dsCOPERNICUS.latitude.max())\n",
    "print(dsCOPERNICUS.longitude.min(), dsCOPERNICUS.longitude.max())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (flake)",
   "language": "python",
   "name": "flake-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
