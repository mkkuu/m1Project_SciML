{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohérence spatio-temporelle\n",
    "\n",
    "Nous souhaitons nous assurer que les données décrivent un même phénomène physique sur une grille spatiale et temporelle cohérente, et cela, en maintenant une continuité et une exploitabilité pour l’analyse et la modélisation.\n",
    "\n",
    "## 1. Cohérence temporelle\n",
    "\n",
    "### 1.1 Vérifier la structure du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 3652)> Size: 29kB\n",
      "array(['2010-01-01T00:00:00.000000000', '2010-01-02T00:00:00.000000000',\n",
      "       '2010-01-03T00:00:00.000000000', ..., '2019-12-29T00:00:00.000000000',\n",
      "       '2019-12-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000'],\n",
      "      shape=(3652,), dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 29kB 2010-01-01 2010-01-02 ... 2019-12-31\n",
      "Attributes:\n",
      "    long_name:     Time\n",
      "    delta_t:       0000-00-01 00:00:00\n",
      "    avg_period:    0000-00-01 00:00:00\n",
      "    axis:          T\n",
      "    actual_range:  [76701. 77065.]\n",
      "{'dtype': dtype('float64'), 'zlib': False, 'szip': False, 'zstd': False, 'bzip2': False, 'blosc': False, 'shuffle': False, 'complevel': 0, 'fletcher32': False, 'contiguous': True, 'chunksizes': None, 'source': '/home/mkkuu/Documents/GitHub/m1Project_SciML/data/raw/merged/sstNOAA20102019.nc', 'original_shape': (3652,), '_FillValue': np.float64(nan), 'units': 'days since 1800-01-01', 'calendar': 'proleptic_gregorian'}\n",
      "\n",
      "***************************************\n",
      "\n",
      "<xarray.DataArray 'time' (time: 3654)> Size: 29kB\n",
      "array(['2010-01-01T00:00:00.000000000', '2010-01-02T00:00:00.000000000',\n",
      "       '2010-01-03T00:00:00.000000000', ..., '2019-12-30T00:00:00.000000000',\n",
      "       '2019-12-31T00:00:00.000000000', '2020-01-01T00:00:00.000000000'],\n",
      "      shape=(3654,), dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 29kB 2010-01-01 2010-01-02 ... 2020-01-01\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      Time\n",
      "    axis:           T\n",
      "{'dtype': dtype('float64'), 'zlib': False, 'szip': False, 'zstd': False, 'bzip2': False, 'blosc': False, 'shuffle': False, 'complevel': 0, 'fletcher32': False, 'contiguous': True, 'chunksizes': None, 'source': '/home/mkkuu/Documents/GitHub/m1Project_SciML/data/raw/merged/sstCOPERNICUS20102019.nc', 'original_shape': (3654,), '_FillValue': np.float64(nan), 'units': 'seconds since 1970-01-01', 'calendar': 'gregorian'}\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "\n",
    "dsNOAA = xr.open_dataset(\"data/raw/merged/sstNOAA20102019.nc\")\n",
    "dsCOPERNICUS = xr.open_dataset(\"data/raw/merged/sstCOPERNICUS20102019.nc\")\n",
    "\n",
    "print(dsNOAA.time)\n",
    "print(dsNOAA.time.encoding)\n",
    "\n",
    "print(\"\\n***************************************\\n\")\n",
    "\n",
    "print(dsCOPERNICUS.time)\n",
    "print(dsCOPERNICUS.time.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparant les axes temporels des deux jeux de données, on constate un léger décalage de borne temporelle. Copernicus incluant un jour supplémentaire au-delà de l'année 2019. Pour garantir la cohérence temporelle stricte entre les sources, nous allons simplement restreindre le jeu de donnée COPERNICUS à la période commune 2010-2019.\n",
    "\n",
    "On note également une différence de standard de calendrier, NOAA utilisant le calendrier grégorien proleptique et COPERNICUS le grégorien simple. Cela ne fait aucune différence pour les années suivant 1582 d'après nos recherches. Aucun traitement ne sera réalisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "<xarray.DataArray 'time' (time: 3653)> Size: 29kB\n",
      "array(['2010-01-01T00:00:00.000000000', '2010-01-02T00:00:00.000000000',\n",
      "       '2010-01-03T00:00:00.000000000', ..., '2019-12-29T00:00:00.000000000',\n",
      "       '2019-12-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000'],\n",
      "      shape=(3653,), dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 29kB 2010-01-01 2010-01-02 ... 2019-12-31\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      Time\n",
      "    axis:           T\n"
     ]
    }
   ],
   "source": [
    "print(dsNOAA.time.equals(dsCOPERNICUS.time)) # False, confirmant l'inégalité des bornes temporelles\n",
    "\n",
    "dsCOPERNICUS = dsCOPERNICUS.sel(time=slice(\"2010-01-01\", \"2019-12-31\")) # We apply slicing to ensure the same time range\n",
    "\n",
    "print(dsNOAA.time.equals(dsCOPERNICUS.time)) # We check again after slicing\n",
    "\n",
    "print(dsCOPERNICUS.time) # We print the time coordinate after slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Continuité temporelle\n",
    "\n",
    "On constate toujours une inégalité entre le nombre de points temporels dans les deux jeux de données. Il semblerait qu'il y ait toujours un point temporel supplémentaire (nous voulons avoir 3652 points correspondant à 8 années à 365 jours et 2 années bissextiles à 366 jours, soit 3652 jours). Explorons l'hypothèse d'un éventuel doublon en vérifiant le pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimedeltaArray>\n",
      "['1 days']\n",
      "Length: 1, dtype: timedelta64[ns] <TimedeltaArray>\n",
      "['1 days', '0 days']\n",
      "Length: 2, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dtNOAA = pd.to_datetime(dsNOAA.time.values).to_series().diff().dropna().unique()\n",
    "dtCOPERNICUS  = pd.to_datetime(dsCOPERNICUS.time.values).to_series().diff().dropna().unique()\n",
    "\n",
    "# For each dataset :\n",
    "#   - convert time values to pandas datetime\n",
    "#   - convert it to a pandas Series to manipulate it easily\n",
    "#   - compute the difference between each time value and the previous one\n",
    "#   - drop the NaT value resulting from the diff operation\n",
    "#   - get the unique values of the resulting time differences\n",
    "\n",
    "print(dtNOAA, dtCOPERNICUS) # Print delta times for both datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultant de notre test, on constate bel et bien la présence d'un doublon dans le jeu de donnée COPERNICUS. Pourquoi ? Car on remarque la présence de l'élément '0 days' dans la liste en retour de notre opération, signifiant qu'il existe 2 points SST pour un jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01   0 days\n",
      "dtype: timedelta64[ns]\n",
      "DatetimeIndex(['2015-01-01'], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "time = pd.to_datetime(dsCOPERNICUS.time.values) # We instanciate a pandas datetime object from the time values of the COPERNICUS dataset\n",
    "dt = time.to_series().diff() # We convert it to a pandas Series and compute the time differences between each value and the previous one\n",
    "\n",
    "duplicates = dt[dt == pd.Timedelta(0)] # We filter the delta times to get only the duplicates of 0 timedelta\n",
    "print(duplicates) \n",
    "\n",
    "idx = duplicates.index # We get the index corresponding to the duplicate time values so it returns us the days where there are duplicates\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jour présent 2 fois est donc le 01 janvier 2015. En effet, ayant télécharger manuellement depuis le site officiel sur les bornes 01/01/2010-01/01/2015 puis dans un second temps 01/01/2015-01/01/2020, cela a généré un doublon à la fusion des deux fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimedeltaArray>\n",
      "['1 days']\n",
      "Length: 1, dtype: timedelta64[ns]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "index = np.unique(dsCOPERNICUS.time, return_index=True)[1] # We get the unique time values and their corresponding indices\n",
    "dsCOPERNICUS = dsCOPERNICUS.isel(time=index) # We select only the unique time values using their indices\n",
    "\n",
    "print(pd.to_datetime(dsCOPERNICUS.time.values).to_series().diff().dropna().unique()) # We confirm there are no more duplicate time values by computing the delta times again\n",
    "print(dsCOPERNICUS.time.equals(dsNOAA.time)) # We test the equality of the time bounds again after removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comparaison sur les axes temporelles des 2 jeux de données retourne True. Nous présumons ainsi la cohérence temporelle vérifiée et les fichiers bien alignés temporellement.\n",
    "\n",
    "## 2. Cohérence spatiale\n",
    "\n",
    "### 2.1 Définition de la grille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenMappingWarningOnValuesAccess({'time': 3652, 'lat': 28, 'lon': 12})\n",
      "<xarray.DataArray 'lat' ()> Size: 4B\n",
      "array(-1.875, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Latitude\n",
      "    standard_name:  latitude\n",
      "    units:          degrees_north\n",
      "    actual_range:   [-89.875  89.875]\n",
      "    axis:           Y <xarray.DataArray 'lat' ()> Size: 4B\n",
      "array(4.875, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Latitude\n",
      "    standard_name:  latitude\n",
      "    units:          degrees_north\n",
      "    actual_range:   [-89.875  89.875]\n",
      "    axis:           Y\n",
      "<xarray.DataArray 'lon' ()> Size: 4B\n",
      "array(48.125, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Longitude\n",
      "    standard_name:  longitude\n",
      "    units:          degrees_east\n",
      "    actual_range:   [1.25000e-01 3.59875e+02]\n",
      "    axis:           X <xarray.DataArray 'lon' ()> Size: 4B\n",
      "array(50.875, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Longitude\n",
      "    standard_name:  longitude\n",
      "    units:          degrees_east\n",
      "    actual_range:   [1.25000e-01 3.59875e+02]\n",
      "    axis:           X\n",
      "\n",
      "***************************************\n",
      "\n",
      "FrozenMappingWarningOnValuesAccess({'time': 3652, 'latitude': 60, 'longitude': 140})\n",
      "<xarray.DataArray 'latitude' ()> Size: 4B\n",
      "array(48.02501, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  latitude\n",
      "    long_name:      Latitude\n",
      "    units:          degrees_north\n",
      "    unit_long:      Degrees North\n",
      "    axis:           Y <xarray.DataArray 'latitude' ()> Size: 4B\n",
      "array(50.97501, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  latitude\n",
      "    long_name:      Latitude\n",
      "    units:          degrees_north\n",
      "    unit_long:      Degrees North\n",
      "    axis:           Y\n",
      "<xarray.DataArray 'longitude' ()> Size: 4B\n",
      "array(-4.9750004, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  longitude\n",
      "    long_name:      Longitude\n",
      "    units:          degrees_east\n",
      "    unit_long:      Degrees East\n",
      "    axis:           X <xarray.DataArray 'longitude' ()> Size: 4B\n",
      "array(1.975, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  longitude\n",
      "    long_name:      Longitude\n",
      "    units:          degrees_east\n",
      "    unit_long:      Degrees East\n",
      "    axis:           X\n"
     ]
    }
   ],
   "source": [
    "print(dsNOAA.dims)\n",
    "print(dsNOAA.lat.min(), dsNOAA.lat.max())\n",
    "print(dsNOAA.lon.min(), dsNOAA.lon.max())\n",
    "\n",
    "print(\"\\n***************************************\\n\")\n",
    "\n",
    "print(dsCOPERNICUS.dims)\n",
    "print(dsCOPERNICUS.latitude.min(), dsCOPERNICUS.latitude.max())\n",
    "print(dsCOPERNICUS.longitude.min(), dsCOPERNICUS.longitude.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Première différence majeure constatée entre les deux de jeux de données : la résolution spatiale. En effet, on constate que NOAA possède une grille de 28 * 12 (latitude*longitude) points par jour, soit 336 points. Le jeu de donnée COPERNICUS va, quant à lui, posséder une grille de 60 * 140 points par jour, soit 8400 points.\n",
    "\n",
    "### 2.2 Résolution spatiale\n",
    "\n",
    "La résolution spatiale correspond à la distance minimale, exprimée en unités de carte (ici les degrés), qui sépare les valeurs x uniques ou les valeurs y uniques dans les coordonnées d'entité. Une résolution plus fine (plus petite valeur) permet une meilleure précision des coordonnées, mais peut entraîner une baisse des performances en raison d'une utilisation accrue du disque et d'un nombre plus élevé d'opérations d'E/S (pour entrée/sortie) ce qui entraîne souvent des périodes d'inactivités du CPU. Déterminons les résolutions des 2 jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'lat' ()> Size: 4B\n",
      "array(0.25, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Latitude\n",
      "    standard_name:  latitude\n",
      "    units:          degrees_north\n",
      "    actual_range:   [-89.875  89.875]\n",
      "    axis:           Y <xarray.DataArray 'lon' ()> Size: 4B\n",
      "array(0.25, dtype=float32)\n",
      "Attributes:\n",
      "    long_name:      Longitude\n",
      "    standard_name:  longitude\n",
      "    units:          degrees_east\n",
      "    actual_range:   [1.25000e-01 3.59875e+02]\n",
      "    axis:           X\n",
      "<xarray.DataArray 'latitude' ()> Size: 4B\n",
      "array(0.05000001, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  latitude\n",
      "    long_name:      Latitude\n",
      "    units:          degrees_north\n",
      "    unit_long:      Degrees North\n",
      "    axis:           Y <xarray.DataArray 'longitude' ()> Size: 4B\n",
      "array(0.05, dtype=float32)\n",
      "Attributes:\n",
      "    standard_name:  longitude\n",
      "    long_name:      Longitude\n",
      "    units:          degrees_east\n",
      "    unit_long:      Degrees East\n",
      "    axis:           X\n"
     ]
    }
   ],
   "source": [
    "# We check spatial resolution for both datasets with the following method :\n",
    "#   - compute the difference between each latitude/longitude value and the previous one\n",
    "#   - compute the mean of these differences to get the average spatial resolution\n",
    "\n",
    "# NOAA\n",
    "\n",
    "latResNOAA = dsNOAA.lat.diff(\"lat\").mean()\n",
    "lonResNOAA = dsNOAA.lon.diff(\"lon\").mean()\n",
    "print(latResNOAA, lonResNOAA)\n",
    "\n",
    "# COPERNICUS\n",
    "\n",
    "latResCopernicus = dsCOPERNICUS.latitude.diff(\"latitude\").mean()\n",
    "lonResCopernicus = dsCOPERNICUS.longitude.diff(\"longitude\").mean()\n",
    "print(latResCopernicus, lonResCopernicus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats coïncident avec notre constat précédent, la résolution spatiale (équivalent au pas temporel pour les coordonnées géographiques) du jeu de donnée COPERNICUS est de 0.05° contre 0.25° pour NOAA, soit 5 fois supérieur. On ne peut pour le moment pas établir quelconques affirmations, cela pourrait simplement signifier que COPERNICUS a plus interpolé que NOAA ce qui se répercuterait sur l'apprentissage des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Orientation des axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'lat' ()> Size: 1B\n",
      "array(True)\n",
      "Attributes:\n",
      "    long_name:      Latitude\n",
      "    standard_name:  latitude\n",
      "    units:          degrees_north\n",
      "    actual_range:   [-89.875  89.875]\n",
      "    axis:           Y\n",
      "<xarray.DataArray 'latitude' ()> Size: 1B\n",
      "array(True)\n",
      "Attributes:\n",
      "    standard_name:  latitude\n",
      "    long_name:      Latitude\n",
      "    units:          degrees_north\n",
      "    unit_long:      Degrees North\n",
      "    axis:           Y\n",
      "<xarray.DataArray 'lon' ()> Size: 1B\n",
      "array(True)\n",
      "Attributes:\n",
      "    long_name:      Longitude\n",
      "    standard_name:  longitude\n",
      "    units:          degrees_east\n",
      "    actual_range:   [1.25000e-01 3.59875e+02]\n",
      "    axis:           X\n",
      "<xarray.DataArray 'longitude' ()> Size: 1B\n",
      "array(True)\n",
      "Attributes:\n",
      "    standard_name:  longitude\n",
      "    long_name:      Longitude\n",
      "    units:          degrees_east\n",
      "    unit_long:      Degrees East\n",
      "    axis:           X\n"
     ]
    }
   ],
   "source": [
    "# We check how the latitude values are ordered (ascending or descending) for both datasets\n",
    "\n",
    "print((dsNOAA.lat.diff(\"lat\") > 0).all())\n",
    "print((dsCOPERNICUS.latitude.diff(\"latitude\") > 0).all()) \n",
    "\n",
    "# Same for longitude values\n",
    "\n",
    "print((dsNOAA.lon.diff(\"lon\") > 0).all())\n",
    "print((dsCOPERNICUS.longitude.diff(\"longitude\") > 0).all()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après vérification de l'orientation des axes, les tests réalisés indiquent que, pour les deux sources, les latitudes sont ordonnées de manière croissante, correspondant à une progression du Sud vers le Nord, les longitudes sont également croissantes, traduisant une orientation de l’Ouest vers l’Est. Cette convention est cohérente avec les standards usuels et ne nécessite aucune réorientation supplémentaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Stabilité de la grille dans le temps\n",
    "\n",
    "Une information importante à vérifier est la stabilité de la grille en fonction du temps. L'idéal serait d'avoir une grille indépendante temporellement, soit une grille dont les points restent identiques d'un point temporel à l'autre. Déterminons si la grille est stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(dsNOAA.isel(time=0).lat.equals(dsNOAA.isel(time=-1).lat)) # We check if latitude values are the same for the first and last time steps in NOAA dataset\n",
    "print(dsCOPERNICUS.isel(time=0).latitude.equals(dsCOPERNICUS.isel(time=-1).latitude))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À première vue, le fait que les tests nous retournent False devrait affirmer que la grille est dépendant du temps. Or, l'égalité testée avec la méthode equals() réalise en réalité un test d'égalité stricte (mêmes valeurs, même dtype, mêmes attributs, même ordre et aucune tolérance numérique). Ce résultat peut donc venir de détails subtils (3.0000001° != 3°) et ne suffit pas à affirmer la dépendance temporelle. Réalisons quelques tests supplémentaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "\n",
      "***************************************\n",
      "\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Here we compare like the previous check but with a tolerance for floating point precision issues thanks to numpy allclose function\n",
    "\n",
    "# NOAA\n",
    "\n",
    "print(np.allclose(\n",
    "    dsNOAA.isel(time=0).lat.values,\n",
    "    dsNOAA.isel(time=-1).lat.values\n",
    "))\n",
    "\n",
    "print(np.allclose(\n",
    "    dsNOAA.isel(time=0).lon.values,\n",
    "    dsNOAA.isel(time=-1).lon.values\n",
    "))\n",
    "\n",
    "print(\"\\n***************************************\\n\")\n",
    "\n",
    "# COPERNICUS\n",
    "\n",
    "print(np.allclose(\n",
    "    dsCOPERNICUS.isel(time=0).latitude.values,\n",
    "    dsCOPERNICUS.isel(time=-1).latitude.values\n",
    "))\n",
    "\n",
    "print(np.allclose(\n",
    "    dsCOPERNICUS.isel(time=0).longitude.values,\n",
    "    dsCOPERNICUS.isel(time=-1).longitude.values\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ajoutant un degré de tolérance, notre test précédent retourne désormais True. Cherchons si de micro-variabilités sont bien les causes de l'échec du précédent test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'long_name': 'Latitude', 'standard_name': 'latitude', 'units': 'degrees_north', 'actual_range': array([-89.875,  89.875], dtype=float32), 'axis': 'Y'}\n",
      "{'long_name': 'Latitude', 'standard_name': 'latitude', 'units': 'degrees_north', 'actual_range': array([-89.875,  89.875], dtype=float32), 'axis': 'Y'}\n",
      "{'long_name': 'Longitude', 'standard_name': 'longitude', 'units': 'degrees_east', 'actual_range': array([1.25000e-01, 3.59875e+02], dtype=float32), 'axis': 'X'}\n",
      "{'long_name': 'Longitude', 'standard_name': 'longitude', 'units': 'degrees_east', 'actual_range': array([1.25000e-01, 3.59875e+02], dtype=float32), 'axis': 'X'}\n",
      "\n",
      "***************************************\n",
      "\n",
      "{'standard_name': 'latitude', 'long_name': 'Latitude', 'units': 'degrees_north', 'unit_long': 'Degrees North', 'axis': 'Y'}\n",
      "{'standard_name': 'latitude', 'long_name': 'Latitude', 'units': 'degrees_north', 'unit_long': 'Degrees North', 'axis': 'Y'}\n",
      "{'standard_name': 'longitude', 'long_name': 'Longitude', 'units': 'degrees_east', 'unit_long': 'Degrees East', 'axis': 'X'}\n",
      "{'standard_name': 'longitude', 'long_name': 'Longitude', 'units': 'degrees_east', 'unit_long': 'Degrees East', 'axis': 'X'}\n"
     ]
    }
   ],
   "source": [
    "# NOAA\n",
    "\n",
    "print(dsNOAA.isel(time=0).lat.attrs)\n",
    "print(dsNOAA.isel(time=-1).lat.attrs)\n",
    "\n",
    "print(dsNOAA.isel(time=0).lon.attrs)\n",
    "print(dsNOAA.isel(time=-1).lon.attrs)\n",
    "\n",
    "print(\"\\n***************************************\\n\")\n",
    "\n",
    "# COPERNICUS\n",
    "\n",
    "print(dsCOPERNICUS.isel(time=0).latitude.attrs)\n",
    "print(dsCOPERNICUS.isel(time=-1).latitude.attrs)\n",
    "\n",
    "print(dsCOPERNICUS.isel(time=0).longitude.attrs)\n",
    "print(dsCOPERNICUS.isel(time=-1).longitude.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate une égalité stricte pour chacun des valeurs des dictionnaires d'attributs des jeux de données. On écarte donc la faute à un changement dans les attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lat',)\n",
      "('latitude',)\n",
      "('lon',)\n",
      "('longitude',)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Latitude dims\n",
    "\n",
    "print(dsNOAA.lat.dims)\n",
    "print(dsCOPERNICUS.latitude.dims)\n",
    "\n",
    "print(\"\\n***************************************\\n\")\n",
    "\n",
    "# Longitude dims\n",
    "\n",
    "print(dsNOAA.lon.dims)\n",
    "print(dsCOPERNICUS.longitude.dims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce dernier test nous affirme que les dimensions de latitude et longitude dépendent uniquement d'elle-même et non du temps. On peut donc affirmer que la grille est indépendante temporellement et que la cause du premier échec est dû à des micro-variances de valeurs de latitude ou longitude pour un même point d'un temps t à t+x."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (flake)",
   "language": "python",
   "name": "flake-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
